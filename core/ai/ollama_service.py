"""
Service Ollama pour JARVIS
Gestion des mod√®les LLM locaux pour la planification et l'analyse
"""
import asyncio
import json
import time
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from enum import Enum
import ollama
from loguru import logger

class ModelType(Enum):
    """Types de mod√®les disponibles"""
    VISION = "vision"
    CODING = "coding"
    PLANNING = "planning"
    AUTOCOMPLETE = "autocomplete"
    GENERAL = "general"

@dataclass
class ModelConfig:
    """Configuration d'un mod√®le"""
    name: str
    type: ModelType
    context_length: int
    temperature: float
    system_prompt: str = ""
    available: bool = False
    
class OllamaModels:
    """Configuration des mod√®les Ollama pour JARVIS"""
    
    MODELS = {
        ModelType.VISION: ModelConfig(
            name="llava:7b",
            type=ModelType.VISION,
            context_length=4096,
            temperature=0.1,
            system_prompt="Tu es JARVIS, un assistant IA qui peut analyser des captures d'√©cran. Analyse les images avec pr√©cision et d√©cris les √©l√©ments d'interface utilisateur visibles."
        ),
        ModelType.PLANNING: ModelConfig(
            name="qwen2.5-coder:7b",
            type=ModelType.PLANNING,
            context_length=8192,
            temperature=0.2,
            system_prompt="Tu es JARVIS, un assistant IA qui planifie des actions pour contr√¥ler un ordinateur. Donne des instructions claires et s√©curis√©es pour accomplir les t√¢ches demand√©es."
        ),
        ModelType.CODING: ModelConfig(
            name="deepseek-coder:6.7b",
            type=ModelType.CODING,
            context_length=4096,
            temperature=0.1,
            system_prompt="Tu es un assistant de programmation expert. Fournis du code Python optimis√© et bien document√©."
        ),
        ModelType.AUTOCOMPLETE: ModelConfig(
            name="deepseek-coder:1.3b",
            type=ModelType.AUTOCOMPLETE,
            context_length=2048,
            temperature=0.0,
            system_prompt="Complete le texte de mani√®re naturelle et contextuelle."
        ),
        ModelType.GENERAL: ModelConfig(
            name="llama3.2:3b",
            type=ModelType.GENERAL,
            context_length=2048,
            temperature=0.3,
            system_prompt="Tu es JARVIS, un assistant IA intelligent et utile. R√©ponds de mani√®re concise et professionnelle."
        )
    }

@dataclass
class GenerationRequest:
    """Requ√™te de g√©n√©ration"""
    model_type: ModelType
    prompt: str
    system_prompt: Optional[str] = None
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    context: Optional[Dict[str, Any]] = None
    images: Optional[List[str]] = None  # Base64 encoded images
    
@dataclass
class GenerationResponse:
    """R√©ponse de g√©n√©ration"""
    content: str
    model_used: str
    tokens_generated: int
    generation_time: float
    success: bool
    error: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class OllamaService:
    """Service principal Ollama"""
    
    def __init__(self):
        self.models: Dict[ModelType, ModelConfig] = OllamaModels.MODELS.copy()
        self.client = None
        self.is_available = False
        self.stats = {
            "requests_total": 0,
            "requests_success": 0,
            "requests_failed": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }
        
        logger.info("ü§ñ Service Ollama initialis√©")
    
    async def initialize(self):
        """Initialise le service Ollama"""
        try:
            # V√©rifier la disponibilit√© d'Ollama
            available_models = ollama.list()
            model_names = [model['name'] for model in available_models['models']]
            
            logger.info(f"üìö Mod√®les Ollama disponibles: {len(model_names)}")
            
            # V√©rifier quels mod√®les JARVIS sont disponibles
            for model_type, config in self.models.items():
                if config.name in model_names:
                    config.available = True
                    logger.success(f"‚úÖ {config.name} ({model_type.value}) disponible")
                else:
                    logger.warning(f"‚ö†Ô∏è  {config.name} ({model_type.value}) non disponible")
            
            # T√©l√©charger les mod√®les manquants critiques
            await self._ensure_critical_models()
            
            self.is_available = any(config.available for config in self.models.values())
            
            if self.is_available:
                logger.success("‚úÖ Service Ollama pr√™t")
            else:
                logger.error("‚ùå Aucun mod√®le Ollama disponible")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation Ollama: {e}")
            self.is_available = False
    
    async def _ensure_critical_models(self):
        """S'assure que les mod√®les critiques sont disponibles"""
        critical_models = [ModelType.PLANNING, ModelType.GENERAL]
        
        for model_type in critical_models:
            config = self.models[model_type]
            if not config.available:
                try:
                    logger.info(f"üîÑ T√©l√©chargement du mod√®le critique: {config.name}")
                    ollama.pull(config.name)
                    config.available = True
                    logger.success(f"‚úÖ {config.name} t√©l√©charg√© avec succ√®s")
                except Exception as e:
                    logger.error(f"‚ùå Erreur t√©l√©chargement {config.name}: {e}")
    
    async def generate(self, request: GenerationRequest) -> GenerationResponse:
        """G√©n√®re une r√©ponse avec Ollama"""
        if not self.is_available:
            return GenerationResponse(
                content="",
                model_used="none",
                tokens_generated=0,
                generation_time=0.0,
                success=False,
                error="Service Ollama non disponible"
            )
        
        model_config = self.models.get(request.model_type)
        if not model_config or not model_config.available:
            # Fallback vers un mod√®le disponible
            fallback_config = self._get_fallback_model()
            if not fallback_config:
                return GenerationResponse(
                    content="",
                    model_used="none",
                    tokens_generated=0,
                    generation_time=0.0,
                    success=False,
                    error=f"Mod√®le {request.model_type.value} non disponible"
                )
            model_config = fallback_config
        
        start_time = time.time()
        self.stats["requests_total"] += 1
        
        try:
            # Pr√©parer les param√®tres
            system_prompt = request.system_prompt or model_config.system_prompt
            temperature = request.temperature if request.temperature is not None else model_config.temperature
            
            # Construire le prompt final
            if system_prompt:
                full_prompt = f"<system>{system_prompt}</system>\n\n{request.prompt}"
            else:
                full_prompt = request.prompt
            
            # Param√®tres de g√©n√©ration
            generate_params = {
                "model": model_config.name,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_ctx": model_config.context_length
                }
            }
            
            # Ajouter les images pour les mod√®les de vision
            if request.images and model_config.type == ModelType.VISION:
                generate_params["images"] = request.images
            
            # Limite de tokens si sp√©cifi√©e
            if request.max_tokens:
                generate_params["options"]["num_predict"] = request.max_tokens
            
            # G√©n√©ration
            response = ollama.generate(**generate_params)
            
            generation_time = time.time() - start_time
            
            # Extraire les statistiques
            tokens_generated = response.get("eval_count", 0)
            
            # Mettre √† jour les stats
            self.stats["requests_success"] += 1
            self.stats["total_tokens"] += tokens_generated
            self.stats["total_time"] += generation_time
            
            result = GenerationResponse(
                content=response["response"],
                model_used=model_config.name,
                tokens_generated=tokens_generated,
                generation_time=generation_time,
                success=True,
                metadata={
                    "model_type": request.model_type.value,
                    "prompt_tokens": response.get("prompt_eval_count", 0),
                    "total_duration": response.get("total_duration", 0)
                }
            )
            
            logger.debug(f"ü§ñ G√©n√©ration r√©ussie: {tokens_generated} tokens en {generation_time:.2f}s")
            return result
            
        except Exception as e:
            generation_time = time.time() - start_time
            self.stats["requests_failed"] += 1
            
            logger.error(f"‚ùå Erreur g√©n√©ration Ollama: {e}")
            
            return GenerationResponse(
                content="",
                model_used=model_config.name,
                tokens_generated=0,
                generation_time=generation_time,
                success=False,
                error=str(e)
            )
    
    def _get_fallback_model(self) -> Optional[ModelConfig]:
        """R√©cup√®re un mod√®le de fallback disponible"""
        # Ordre de pr√©f√©rence pour les fallbacks
        fallback_order = [ModelType.GENERAL, ModelType.PLANNING, ModelType.CODING]
        
        for model_type in fallback_order:
            config = self.models.get(model_type)
            if config and config.available:
                return config
        
        # Dernier recours: n'importe quel mod√®le disponible
        for config in self.models.values():
            if config.available:
                return config
        
        return None
    
    # M√©thodes de commodit√© pour diff√©rents types de t√¢ches
    
    async def plan_action(self, task_description: str, context: Dict[str, Any] = None) -> GenerationResponse:
        """Planifie une action √† effectuer"""
        context_str = ""
        if context:
            context_str = f"\nContexte: {json.dumps(context, indent=2)}"
        
        prompt = f"""T√¢che √† accomplir: {task_description}{context_str}

Planifie les √©tapes d√©taill√©es pour accomplir cette t√¢che en utilisant les outils disponibles:
- Vision (capture et analyse d'√©cran)
- Contr√¥le souris (clic, d√©placement, glisser-d√©poser)
- Contr√¥le clavier (frappe, raccourcis)
- D√©tection d'applications

Fournis un plan structur√© avec:
1. Analyse de la situation actuelle
2. √âtapes s√©quentielles √† suivre
3. Actions sp√©cifiques pour chaque √©tape
4. V√©rifications √† effectuer

Sois pr√©cis et s√©curis√© dans tes recommandations."""
        
        request = GenerationRequest(
            model_type=ModelType.PLANNING,
            prompt=prompt,
            context=context
        )
        
        return await self.generate(request)
    
    async def analyze_screen(self, image_base64: str, objective: str = None) -> GenerationResponse:
        """Analyse une capture d'√©cran"""
        objective_text = f"\nObjectif: {objective}" if objective else ""
        
        prompt = f"""Analyse cette capture d'√©cran en d√©tail.{objective_text}

D√©cris:
1. Le type d'application ou de page visible
2. Les √©l√©ments d'interface principaux (boutons, menus, champs)
3. Le contenu textuel important
4. L'√©tat actuel de l'interface
5. Les actions possibles pour l'utilisateur

Sois pr√©cis et d√©taill√© dans ton analyse."""
        
        request = GenerationRequest(
            model_type=ModelType.VISION,
            prompt=prompt,
            images=[image_base64]
        )
        
        return await self.generate(request)
    
    async def generate_code(self, task: str, language: str = "python") -> GenerationResponse:
        """G√©n√®re du code pour une t√¢che"""
        prompt = f"""G√©n√®re du code {language} pour accomplir cette t√¢che: {task}

Exigences:
- Code propre et bien document√©
- Gestion d'erreurs appropri√©e
- Type hints si applicable
- Commentaires explicatifs
- Code fonctionnel et test√©

Fournis uniquement le code, sans explications suppl√©mentaires."""
        
        request = GenerationRequest(
            model_type=ModelType.CODING,
            prompt=prompt,
            temperature=0.1
        )
        
        return await self.generate(request)
    
    async def autocomplete_text(self, text: str, context: str = "") -> GenerationResponse:
        """Auto-compl√©tion de texte"""
        context_text = f"Contexte: {context}\n\n" if context else ""
        
        prompt = f"""{context_text}Texte √† compl√©ter: {text}

Complete ce texte de mani√®re naturelle et contextuelle. Fournis seulement la suite du texte, sans r√©p√©ter le texte original."""
        
        request = GenerationRequest(
            model_type=ModelType.AUTOCOMPLETE,
            prompt=prompt,
            max_tokens=100,
            temperature=0.0
        )
        
        return await self.generate(request)
    
    async def chat(self, message: str, conversation_history: List[Dict[str, str]] = None) -> GenerationResponse:
        """Discussion g√©n√©rale avec JARVIS"""
        history_text = ""
        if conversation_history:
            for entry in conversation_history[-5:]:  # Garder les 5 derniers messages
                role = entry.get("role", "user")
                content = entry.get("content", "")
                history_text += f"{role}: {content}\n"
        
        prompt = f"""{history_text}user: {message}

R√©ponds de mani√®re naturelle et utile en tant que JARVIS."""
        
        request = GenerationRequest(
            model_type=ModelType.GENERAL,
            prompt=prompt
        )
        
        return await self.generate(request)
    
    # M√©thodes de gestion
    
    def get_available_models(self) -> List[str]:
        """Retourne la liste des mod√®les disponibles"""
        return [config.name for config in self.models.values() if config.available]
    
    def get_model_info(self, model_type: ModelType) -> Optional[Dict[str, Any]]:
        """Retourne les informations d'un mod√®le"""
        config = self.models.get(model_type)
        if not config:
            return None
        
        return {
            "name": config.name,
            "type": config.type.value,
            "context_length": config.context_length,
            "temperature": config.temperature,
            "available": config.available,
            "system_prompt": config.system_prompt[:100] + "..." if len(config.system_prompt) > 100 else config.system_prompt
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du service"""
        stats = self.stats.copy()
        
        if stats["requests_total"] > 0:
            stats["success_rate"] = stats["requests_success"] / stats["requests_total"]
            stats["avg_tokens_per_request"] = stats["total_tokens"] / stats["requests_success"] if stats["requests_success"] > 0 else 0
            stats["avg_time_per_request"] = stats["total_time"] / stats["requests_success"] if stats["requests_success"] > 0 else 0
        
        stats["available_models"] = len(self.get_available_models())
        stats["service_available"] = self.is_available
        
        return stats
    
    def reset_stats(self):
        """Remet √† z√©ro les statistiques"""
        self.stats = {
            "requests_total": 0,
            "requests_success": 0,
            "requests_failed": 0,
            "total_tokens": 0,
            "total_time": 0.0
        }
        logger.info("üìä Statistiques Ollama remises √† z√©ro")

# Fonctions utilitaires
async def quick_generate(prompt: str, model_type: ModelType = ModelType.GENERAL) -> str:
    """G√©n√©ration rapide"""
    service = OllamaService()
    await service.initialize()
    
    request = GenerationRequest(model_type=model_type, prompt=prompt)
    response = await service.generate(request)
    
    return response.content if response.success else f"Erreur: {response.error}"

async def quick_chat(message: str) -> str:
    """Chat rapide avec JARVIS"""
    service = OllamaService()
    await service.initialize()
    
    response = await service.chat(message)
    return response.content if response.success else f"Erreur: {response.error}"

if __name__ == "__main__":
    async def test_ollama():
        service = OllamaService()
        await service.initialize()
        
        if not service.is_available:
            print("‚ùå Service Ollama non disponible")
            return
        
        # Test de chat
        print("ü§ñ Test de chat:")
        response = await service.chat("Bonjour JARVIS, peux-tu te pr√©senter?")
        if response.success:
            print(f"JARVIS: {response.content}")
            print(f"üìä {response.tokens_generated} tokens en {response.generation_time:.2f}s")
        
        # Test de planification
        print("\nüìã Test de planification:")
        response = await service.plan_action("Ouvrir le navigateur et aller sur Google")
        if response.success:
            print(f"Plan: {response.content[:200]}...")
        
        # Statistiques
        print("\nüìä Statistiques:")
        stats = service.get_stats()
        print(f"- Requ√™tes: {stats['requests_total']} (succ√®s: {stats['requests_success']})")
        print(f"- Mod√®les disponibles: {stats['available_models']}")
    
    asyncio.run(test_ollama())