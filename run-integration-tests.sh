#!/bin/bash
# Script pour lancer les tests d'int√©gration JARVIS AI
# N√©cessite Docker et les services en cours d'ex√©cution

set -e

echo "üß™ JARVIS AI - Tests d'Int√©gration et Performance"
echo "================================================="

# Variables
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
RESULTS_DIR="$PROJECT_ROOT/test-results"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Cr√©er le r√©pertoire de r√©sultats
mkdir -p "$RESULTS_DIR"

# V√©rifier que Docker est en cours d'ex√©cution
echo "üê≥ V√©rification de Docker..."
if ! docker info > /dev/null 2>&1; then
    echo "‚ùå Docker n'est pas en cours d'ex√©cution!"
    exit 1
fi

# V√©rifier les services JARVIS
echo "üîç V√©rification des services JARVIS..."
REQUIRED_SERVICES=(
    "jarvis_brain"
    "jarvis_tts"
    "jarvis_stt"
    "jarvis_system_control"
    "jarvis_terminal"
    "jarvis_mcp_gateway"
    "jarvis_autocomplete"
    "jarvis_redis"
    "jarvis_memory_db"
)

MISSING_SERVICES=()
for service in "${REQUIRED_SERVICES[@]}"; do
    if ! docker ps --format "table {{.Names}}" | grep -q "$service"; then
        MISSING_SERVICES+=("$service")
    fi
done

if [ ${#MISSING_SERVICES[@]} -gt 0 ]; then
    echo "‚ùå Services manquants : ${MISSING_SERVICES[*]}"
    echo "Lancement des services..."
    docker-compose up -d
    echo "‚è≥ Attente du d√©marrage des services (30s)..."
    sleep 30
fi

# Installer les d√©pendances de test si n√©cessaire
if [ ! -d "venv" ]; then
    echo "üì¶ Cr√©ation de l'environnement virtuel..."
    python -m venv venv
fi

# Activer l'environnement virtuel
source venv/bin/activate

echo "üì¶ Installation des d√©pendances de test..."
pip install -q pytest pytest-asyncio pytest-benchmark pytest-cov docker aiohttp websockets requests

# Ex√©cuter les tests d'int√©gration
echo ""
echo "üß™ Lancement des tests d'int√©gration..."
echo "======================================="

# Tests Docker Services
echo ""
echo "üê≥ Tests des services Docker..."
pytest tests/integration/test_docker_services.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/docker-services-$TIMESTAMP.xml" || true

# Tests API Integration
echo ""
echo "üîå Tests d'int√©gration API..."
pytest tests/integration/test_api_integration.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/api-integration-$TIMESTAMP.xml" || true

# Tests WebSocket
echo ""
echo "üåê Tests WebSocket..."
pytest tests/integration/test_websocket_integration.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/websocket-$TIMESTAMP.xml" || true

# Tests Audio Pipeline
echo ""
echo "üéµ Tests pipeline audio..."
pytest tests/integration/test_audio_pipeline.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/audio-pipeline-$TIMESTAMP.xml" || true

# Ex√©cuter les tests de performance
echo ""
echo "‚ö° Lancement des tests de performance..."
echo "======================================="

# Tests de charge Brain API
echo ""
echo "üß† Tests de charge Brain API..."
pytest tests/performance/test_load_brain_api.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/load-brain-api-$TIMESTAMP.xml" || true

# Tests latence audio
echo ""
echo "üé§ Tests latence audio..."
pytest tests/performance/test_audio_latency.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/audio-latency-$TIMESTAMP.xml" || true

# Tests d√©bit WebSocket
echo ""
echo "üìä Tests d√©bit WebSocket..."
pytest tests/performance/test_websocket_throughput.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/websocket-throughput-$TIMESTAMP.xml" || true

# Tests monitoring GPU
echo ""
echo "üéÆ Tests performance GPU monitoring..."
pytest tests/performance/test_gpu_monitoring.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/gpu-monitoring-$TIMESTAMP.xml" || true

# Ex√©cuter les tests de r√©silience
echo ""
echo "üõ°Ô∏è Lancement des tests de r√©silience..."
echo "======================================="

# Tests de coupures r√©seau
echo ""
echo "üîå Tests coupures r√©seau..."
pytest tests/resilience/test_network_failures.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/network-failures-$TIMESTAMP.xml" || true

# Tests de r√©cup√©ration
echo ""
echo "üîÑ Tests r√©cup√©ration apr√®s crash..."
pytest tests/resilience/test_service_recovery.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/service-recovery-$TIMESTAMP.xml" || true

# Tests limites ressources
echo ""
echo "üìà Tests limites ressources..."
pytest tests/resilience/test_resource_limits.py -v --tb=short \
    --junit-xml="$RESULTS_DIR/resource-limits-$TIMESTAMP.xml" || true

# G√©n√©rer le rapport de synth√®se
echo ""
echo "üìä G√©n√©ration du rapport de synth√®se..."
echo "======================================"

# Compter les r√©sultats
TOTAL_TESTS=$(find "$RESULTS_DIR" -name "*-$TIMESTAMP.xml" -exec grep -c 'testcase' {} \; | awk '{sum += $1} END {print sum}')
FAILED_TESTS=$(find "$RESULTS_DIR" -name "*-$TIMESTAMP.xml" -exec grep -c 'failure\|error' {} \; | awk '{sum += $1} END {print sum}')
SUCCESS_RATE=$((100 * (TOTAL_TESTS - FAILED_TESTS) / TOTAL_TESTS))

# Cr√©er le rapport HTML
cat > "$RESULTS_DIR/report-$TIMESTAMP.html" <<EOF
<!DOCTYPE html>
<html>
<head>
    <title>JARVIS AI - Rapport de Tests d'Int√©gration</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background: #0a0a0a; color: #00d4ff; }
        h1 { color: #00ff88; text-align: center; }
        .summary { background: rgba(0,212,255,0.1); padding: 20px; border-radius: 10px; margin: 20px 0; }
        .metric { display: inline-block; margin: 10px 20px; }
        .success { color: #00ff88; }
        .failure { color: #ff6b00; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { padding: 10px; text-align: left; border: 1px solid #00d4ff; }
        th { background: rgba(0,212,255,0.2); }
    </style>
</head>
<body>
    <h1>ü§ñ JARVIS AI - Rapport de Tests d'Int√©gration</h1>
    <div class="summary">
        <h2>üìä R√©sum√©</h2>
        <div class="metric">Total Tests: <strong>$TOTAL_TESTS</strong></div>
        <div class="metric">R√©ussis: <strong class="success">$((TOTAL_TESTS - FAILED_TESTS))</strong></div>
        <div class="metric">√âchou√©s: <strong class="failure">$FAILED_TESTS</strong></div>
        <div class="metric">Taux de R√©ussite: <strong class="$([[ $SUCCESS_RATE -ge 80 ]] && echo "success" || echo "failure")">$SUCCESS_RATE%</strong></div>
        <div class="metric">Date: <strong>$(date)</strong></div>
    </div>
    
    <h2>üß™ R√©sultats D√©taill√©s</h2>
    <table>
        <tr>
            <th>Cat√©gorie</th>
            <th>Fichier de R√©sultats</th>
            <th>Statut</th>
        </tr>
EOF

# Ajouter les r√©sultats au rapport
for result_file in "$RESULTS_DIR"/*-"$TIMESTAMP".xml; do
    if [ -f "$result_file" ]; then
        basename_file=$(basename "$result_file")
        category=$(echo "$basename_file" | sed "s/-$TIMESTAMP.xml//")
        
        # V√©rifier si des tests ont √©chou√©
        if grep -q 'failure\|error' "$result_file"; then
            status="<span class='failure'>‚ùå √âchec</span>"
        else
            status="<span class='success'>‚úÖ Succ√®s</span>"
        fi
        
        echo "        <tr><td>$category</td><td>$basename_file</td><td>$status</td></tr>" >> "$RESULTS_DIR/report-$TIMESTAMP.html"
    fi
done

# Fermer le rapport HTML
cat >> "$RESULTS_DIR/report-$TIMESTAMP.html" <<EOF
    </table>
    
    <h2>üöÄ Prochaines √âtapes</h2>
    <ul>
        <li>Examiner les tests √©chou√©s dans les fichiers XML</li>
        <li>V√©rifier les logs des services pour plus de d√©tails</li>
        <li>Relancer les tests √©chou√©s individuellement</li>
        <li>Optimiser les performances si n√©cessaire</li>
    </ul>
</body>
</html>
EOF

# Afficher le r√©sum√©
echo ""
echo "‚úÖ Tests termin√©s!"
echo "=================="
echo "üìä Total: $TOTAL_TESTS tests"
echo "‚úÖ R√©ussis: $((TOTAL_TESTS - FAILED_TESTS))"
echo "‚ùå √âchou√©s: $FAILED_TESTS"
echo "üìà Taux de r√©ussite: $SUCCESS_RATE%"
echo ""
echo "üìÅ R√©sultats sauvegard√©s dans: $RESULTS_DIR"
echo "üìÑ Rapport HTML: $RESULTS_DIR/report-$TIMESTAMP.html"
echo ""

# Ouvrir le rapport si possible
if command -v xdg-open > /dev/null; then
    xdg-open "$RESULTS_DIR/report-$TIMESTAMP.html"
elif command -v open > /dev/null; then
    open "$RESULTS_DIR/report-$TIMESTAMP.html"
fi

# Code de sortie bas√© sur le taux de r√©ussite
if [ $SUCCESS_RATE -lt 80 ]; then
    echo "‚ö†Ô∏è Taux de r√©ussite inf√©rieur √† 80%!"
    exit 1
else
    echo "üéâ Tests d'int√©gration r√©ussis!"
    exit 0
fi