"""
ü§ñ Module Agent React - JARVIS Brain API
Orchestration autonome d'outils avec framework ReAct
"""

import asyncio
import json
import time
from typing import Dict, List, Optional, Any, Tuple
import logging
from dataclasses import dataclass, asdict
from enum import Enum

from .llm_manager import LLMManager, ModelSelectionStrategy

logger = logging.getLogger(__name__)

class AgentState(Enum):
    IDLE = "idle"
    THINKING = "thinking"
    ACTING = "acting"
    OBSERVING = "observing"
    COMPLETED = "completed"
    ERROR = "error"

@dataclass
class AgentStep:
    step_number: int
    state: AgentState
    thought: str
    action: Optional[str] = None
    action_input: Optional[Dict] = None
    observation: Optional[str] = None
    timestamp: float = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

@dataclass
class AgentExecution:
    task: str
    steps: List[AgentStep]
    final_answer: Optional[str] = None
    status: AgentState = AgentState.IDLE
    start_time: float = None
    end_time: float = None
    total_duration: float = None
    
    def __post_init__(self):
        if self.start_time is None:
            self.start_time = time.time()

class ReactAgent:
    """
    Agent ReAct (Reasoning and Action) pour orchestration autonome d'outils
    Impl√©mente le pattern Think ‚Üí Act ‚Üí Observe
    """
    
    def __init__(self, llm_url: str, memory_manager=None, metacognition=None, persona_manager=None, llm_gateway_url: str = None):
        self.llm_url = llm_url
        self.llm_gateway_url = llm_gateway_url or "http://llm-gateway:5010"
        self.use_gateway = True  # Activer le gateway par d√©faut
        self.memory_manager = memory_manager
        self.metacognition = metacognition
        self.persona_manager = persona_manager
        
        # Configuration
        self.max_iterations = 5
        self.timeout_seconds = 30
        self.debug = True
        
        # √âtat de l'agent
        self.current_execution: Optional[AgentExecution] = None
        self.is_active = False
        
        # Outils disponibles
        self.tools = {}
        self.tool_descriptions = {}
        
        # LLM Manager pour routing intelligent
        self.llm_manager = None
        
        # Statistiques
        self.stats = {
            "total_executions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "tools_used": {},
            "avg_duration": 0.0,
            "avg_steps": 0.0
        }
        
        logger.info("ü§ñ React Agent initialis√© avec Persona Manager" if persona_manager else "ü§ñ React Agent initialis√©")
    
    async def initialize(self):
        """Initialisation asynchrone de l'agent"""
        logger.info("üöÄ Initialisation React Agent...")
        
        # Initialiser LLM Manager
        self.llm_manager = LLMManager(
            gateway_url=self.llm_gateway_url,
            fallback_url=self.llm_url,
            strategy=ModelSelectionStrategy.COMPLEXITY_BASED
        )
        await self.llm_manager.initialize()
        
        # Charger les outils disponibles
        await self._load_tools()
        
        # Tester connexion LLM via manager
        await self._test_llm_connection()
        
        logger.info(f"‚úÖ React Agent pr√™t avec {len(self.tools)} outils et LLM Gateway")
    
    async def shutdown(self):
        """Arr√™t propre de l'agent"""
        logger.info("üõë Arr√™t React Agent...")
        
        if self.current_execution and self.current_execution.status not in [AgentState.COMPLETED, AgentState.ERROR]:
            logger.warning("Arr√™t forc√© pendant une ex√©cution")
            if self.current_execution:
                self.current_execution.status = AgentState.ERROR
        
        # Arr√™t LLM Manager
        if self.llm_manager:
            await self.llm_manager.shutdown()
        
        self._log_final_stats()
    
    async def execute_task(self, task: str, context: Optional[Dict] = None) -> AgentExecution:
        """
        Ex√©cuter une t√¢che avec le framework ReAct
        
        Args:
            task: Description de la t√¢che √† accomplir
            context: Contexte additionnel (utilisateur, historique, etc.)
        
        Returns:
            AgentExecution: R√©sultat de l'ex√©cution avec toutes les √©tapes
        """
        if self.is_active:
            raise RuntimeError("Agent d√©j√† en cours d'ex√©cution")
        
        self.is_active = True
        self.stats["total_executions"] += 1
        
        # Initialiser l'ex√©cution
        execution = AgentExecution(task=task, steps=[])
        self.current_execution = execution
        
        try:
            logger.info(f"üéØ D√©marrage t√¢che: {task}")
            execution.status = AgentState.THINKING
            
            # Boucle ReAct principale
            for iteration in range(self.max_iterations):
                # V√©rifier timeout
                if time.time() - execution.start_time > self.timeout_seconds:
                    raise TimeoutError(f"Timeout apr√®s {self.timeout_seconds}s")
                
                # √âtape THINK (R√©flexion)
                thought = await self._think_step(task, execution.steps, context)
                step = AgentStep(
                    step_number=iteration + 1,
                    state=AgentState.THINKING,
                    thought=thought
                )
                execution.steps.append(step)
                
                # V√©rifier si on a la r√©ponse finale
                if self._is_final_answer(thought):
                    execution.final_answer = self._extract_final_answer(thought)
                    execution.status = AgentState.COMPLETED
                    break
                
                # √âtape ACT (Action)
                action, action_input = await self._act_step(thought)
                if action:
                    step.state = AgentState.ACTING
                    step.action = action
                    step.action_input = action_input
                    
                    # √âtape OBSERVE (Observation)
                    observation = await self._observe_step(action, action_input)
                    step.state = AgentState.OBSERVING
                    step.observation = observation
                    
                    logger.info(f"üîÑ √âtape {iteration + 1}: {action} ‚Üí {observation[:100]}...")
                else:
                    # Pas d'action n√©cessaire, on continue la r√©flexion
                    continue
            
            # Finaliser l'ex√©cution
            if execution.status != AgentState.COMPLETED:
                # G√©n√©rer une r√©ponse finale si pas encore fait
                raw_answer = await self._generate_final_answer(execution)
                # Appliquer le formatage de la persona si disponible
                execution.final_answer = await self._format_with_persona_async(raw_answer, context)
                execution.status = AgentState.COMPLETED
            
            execution.end_time = time.time()
            execution.total_duration = execution.end_time - execution.start_time
            
            self.stats["successful_executions"] += 1
            self._update_stats(execution)
            
            logger.info(f"‚úÖ T√¢che compl√©t√©e en {execution.total_duration:.2f}s avec {len(execution.steps)} √©tapes")
            
        except Exception as e:
            execution.status = AgentState.ERROR
            execution.end_time = time.time()
            execution.total_duration = execution.end_time - execution.start_time
            
            self.stats["failed_executions"] += 1
            logger.error(f"‚ùå √âchec t√¢che: {e}")
            
            # Ajouter l'erreur comme derni√®re observation
            if execution.steps:
                execution.steps[-1].observation = f"Erreur: {str(e)}"
        
        finally:
            self.is_active = False
            self.current_execution = None
        
        return execution
    
    async def _think_step(self, task: str, previous_steps: List[AgentStep], context: Optional[Dict]) -> str:
        """√âtape de r√©flexion - analyser la situation et planifier"""
        
        # Construire le prompt de r√©flexion
        prompt = self._build_thinking_prompt(task, previous_steps, context)
        
        # Utiliser la m√©tacognition pour filtrer si n√©cessaire
        if self.metacognition:
            should_use_llm, reason = self.metacognition.should_activate_llm(prompt, context)
            if not should_use_llm:
                return f"R√©flexion simplifi√©e: {reason}"
        
        # Appeler le LLM pour la r√©flexion
        thought = await self._call_llm(prompt)
        
        return thought
    
    async def _act_step(self, thought: str) -> Tuple[Optional[str], Optional[Dict]]:
        """√âtape d'action - d√©cider quelle action prendre"""
        
        # Analyser la pens√©e pour extraire l'action
        action_match = self._parse_action_from_thought(thought)
        
        if not action_match:
            return None, None
        
        action_name = action_match.get("action")
        action_input = action_match.get("input", {})
        
        # V√©rifier que l'outil existe
        if action_name not in self.tools:
            logger.warning(f"‚ö†Ô∏è Outil inconnu: {action_name}")
            return None, None
        
        return action_name, action_input
    
    async def _observe_step(self, action: str, action_input: Dict) -> str:
        """√âtape d'observation - ex√©cuter l'action et observer le r√©sultat"""
        
        try:
            # Ex√©cuter l'outil
            tool_function = self.tools[action]
            result = await tool_function(action_input)
            
            # Mettre √† jour les statistiques d'utilisation des outils
            self.stats["tools_used"][action] = self.stats["tools_used"].get(action, 0) + 1
            
            return str(result)
            
        except Exception as e:
            error_msg = f"Erreur lors de l'ex√©cution de {action}: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def _build_thinking_prompt(self, task: str, previous_steps: List[AgentStep], context: Optional[Dict]) -> str:
        """Construire le prompt pour l'√©tape de r√©flexion"""
        
        prompt = f"""Tu es JARVIS, un assistant IA autonome. Tu dois accomplir la t√¢che suivante:

T√ÇCHE: {task}

OUTILS DISPONIBLES:
{self._format_tools_description()}

INSTRUCTIONS:
1. R√©fl√©chis √©tape par √©tape √† comment accomplir cette t√¢che
2. Si tu as besoin d'utiliser un outil, indique: Action: [nom_outil] avec Input: [param√®tres]
3. Si tu as assez d'informations pour r√©pondre, indique: R√©ponse finale: [ta_r√©ponse]

"""
        
        # Ajouter l'historique des √©tapes pr√©c√©dentes
        if previous_steps:
            prompt += "√âTAPES PR√âC√âDENTES:\n"
            for step in previous_steps:
                prompt += f"√âtape {step.step_number}: {step.thought}\n"
                if step.action:
                    prompt += f"Action: {step.action} ‚Üí {step.observation}\n"
            prompt += "\n"
        
        # Ajouter le contexte si disponible
        if context:
            user_context = context.get("user_profile", "")
            if user_context:
                prompt += f"CONTEXTE UTILISATEUR: {user_context}\n\n"
        
        prompt += "R√âFLEXION ACTUELLE:"
        
        return prompt
    
    def _format_tools_description(self) -> str:
        """Formater la description des outils disponibles"""
        descriptions = []
        for tool_name, description in self.tool_descriptions.items():
            descriptions.append(f"- {tool_name}: {description}")
        return "\n".join(descriptions)
    
    def _parse_action_from_thought(self, thought: str) -> Optional[Dict]:
        """Parser l'action depuis la pens√©e de l'agent"""
        import re
        
        # Chercher pattern "Action: [nom] avec Input: [params]"
        action_pattern = r"Action:\s*(\w+).*?Input:\s*(.+?)(?:\n|$)"
        match = re.search(action_pattern, thought, re.IGNORECASE | re.DOTALL)
        
        if match:
            action_name = match.group(1).strip()
            input_str = match.group(2).strip()
            
            # Essayer de parser l'input comme JSON
            try:
                action_input = json.loads(input_str)
            except:
                # Si pas JSON, traiter comme string simple
                action_input = {"query": input_str}
            
            return {
                "action": action_name,
                "input": action_input
            }
        
        return None
    
    def _is_final_answer(self, thought: str) -> bool:
        """V√©rifier si la pens√©e contient une r√©ponse finale"""
        final_indicators = [
            "r√©ponse finale:",
            "final answer:",
            "conclusion:",
            "r√©sultat:"
        ]
        
        thought_lower = thought.lower()
        return any(indicator in thought_lower for indicator in final_indicators)
    
    def _extract_final_answer(self, thought: str) -> str:
        """Extraire la r√©ponse finale de la pens√©e"""
        import re
        
        patterns = [
            r"r√©ponse finale:\s*(.+?)(?:\n|$)",
            r"final answer:\s*(.+?)(?:\n|$)",
            r"conclusion:\s*(.+?)(?:\n|$)",
            r"r√©sultat:\s*(.+?)(?:\n|$)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, thought, re.IGNORECASE | re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # Si pas de pattern, prendre la derni√®re phrase
        sentences = thought.split('.')
        return sentences[-1].strip() if sentences else thought
    
    async def _generate_final_answer(self, execution: AgentExecution) -> str:
        """G√©n√©rer une r√©ponse finale bas√©e sur l'ex√©cution"""
        
        # R√©sumer les observations
        observations = []
        for step in execution.steps:
            if step.observation:
                observations.append(step.observation)
        
        if observations:
            summary = " ".join(observations[-3:])  # Derni√®res 3 observations
            return f"Bas√© sur mes recherches: {summary}"
        else:
            return "Je n'ai pas pu rassembler suffisamment d'informations pour r√©pondre compl√®tement √† votre demande."
    
    async def _call_llm(self, prompt: str) -> str:
        """Appeler le LLM local (Ollama)"""
        
        # Appel LLM via Ollama
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": "llama3.2:3b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "max_tokens": 512
                    }
                }
                
                async with session.post(f"{self.llm_url}/api/generate", json=payload) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get("response", "Pas de r√©ponse du LLM")
                    else:
                        logger.warning(f"LLM request failed: {response.status}")
                        return self._fallback_response(prompt)
        except Exception as e:
            logger.warning(f"LLM connection failed: {e}")
            return self._fallback_response(prompt)
    
    def _fallback_response(self, prompt: str) -> str:
        """R√©ponse de secours si LLM indisponible"""
        
        # R√©ponse de secours bas√©e sur le prompt
        if "outils disponibles" in prompt.lower():
            return "Je vais analyser les outils disponibles pour accomplir cette t√¢che. Laisse-moi r√©fl√©chir √† la meilleure approche."
        elif "√©tapes pr√©c√©dentes" in prompt.lower():
            return "En me basant sur les √©tapes pr√©c√©dentes, je pense que je peux maintenant fournir une r√©ponse finale."
        else:
            return "Je r√©fl√©chis √† la meilleure fa√ßon d'aborder cette t√¢che."
    
    async def _test_llm_connection(self):
        """Tester la connexion au LLM"""
        try:
            test_response = await self._call_llm("Test de connexion")
            if test_response:
                logger.info("‚úÖ Connexion LLM test√©e")
            else:
                logger.warning("‚ö†Ô∏è R√©ponse LLM vide")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Connexion LLM √©chou√©e: {e}")
    
    async def _load_tools(self):
        """Charger les outils disponibles"""
        
        # Outils de base pour d√©marrage
        self.tools = {
            "search_web": self._tool_search_web,
            "get_time": self._tool_get_time,
            "calculate": self._tool_calculate,
            "get_weather": self._tool_get_weather,
            "read_file": self._tool_read_file,
            "system_info": self._tool_system_info
        }
        
        self.tool_descriptions = {
            "search_web": "Recherche d'informations sur internet",
            "get_time": "Obtenir l'heure et la date actuelles",
            "calculate": "Effectuer des calculs math√©matiques",
            "get_weather": "Obtenir les informations m√©t√©o",
            "read_file": "Lire le contenu d'un fichier",
            "system_info": "Obtenir des informations syst√®me"
        }
        
        logger.info(f"üîß {len(self.tools)} outils charg√©s")
    
    # === OUTILS IMPL√âMENT√âS ===
    
    async def _tool_search_web(self, params: Dict) -> str:
        """Outil de recherche web (simulation)"""
        query = params.get("query", "")
        await asyncio.sleep(0.2)  # Simule latence r√©seau
        return f"R√©sultats de recherche pour '{query}': Information trouv√©e (simulation)"
    
    async def _tool_get_time(self, params: Dict) -> str:
        """Outil pour obtenir l'heure"""
        import datetime
        now = datetime.datetime.now()
        return f"Il est actuellement {now.strftime('%H:%M:%S')} le {now.strftime('%d/%m/%Y')}"
    
    async def _tool_calculate(self, params: Dict) -> str:
        """Outil de calcul"""
        expression = params.get("expression", "")
        try:
            # S√©curis√©: seulement op√©rations de base
            allowed_chars = set("0123456789+-*/().,")
            if all(c in allowed_chars for c in expression.replace(" ", "")):
                result = eval(expression)
                return f"Le r√©sultat de {expression} est {result}"
            else:
                return "Expression non autoris√©e"
        except:
            return "Erreur dans le calcul"
    
    async def _tool_get_weather(self, params: Dict) -> str:
        """Outil m√©t√©o (simulation)"""
        location = params.get("location", "Paris")
        await asyncio.sleep(0.1)
        return f"M√©t√©o √† {location}: 22¬∞C, ensoleill√© (simulation)"
    
    async def _tool_read_file(self, params: Dict) -> str:
        """Outil lecture de fichier (s√©curis√©)"""
        file_path = params.get("path", "")
        
        # V√©rifications de s√©curit√©
        import os
        from pathlib import Path
        
        try:
            path = Path(file_path).resolve()
            
            # V√©rifier que le fichier existe et est lisible
            if not path.exists():
                return f"Fichier non trouv√©: {file_path}"
            
            if not path.is_file():
                return f"Le chemin n'est pas un fichier: {file_path}"
            
            # V√©rifier les extensions autoris√©es
            allowed_extensions = {'.txt', '.md', '.json', '.yaml', '.yml', '.log'}
            if path.suffix.lower() not in allowed_extensions:
                return f"Type de fichier non autoris√©: {path.suffix}"
            
            # Lire le fichier avec limitation de taille
            max_size = 10 * 1024  # 10KB max
            if path.stat().st_size > max_size:
                return f"Fichier trop volumineux (>{max_size} bytes)"
            
            content = path.read_text(encoding='utf-8', errors='ignore')
            return f"Contenu de {path.name}:\n{content[:1000]}{'...' if len(content) > 1000 else ''}"
            
        except Exception as e:
            return f"Erreur lors de la lecture: {str(e)}"
    
    async def _tool_system_info(self, params: Dict) -> str:
        """Outil informations syst√®me"""
        import platform
        import psutil
        
        info = {
            "os": platform.system(),
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent
        }
        
        return f"Syst√®me: {info['os']}, CPU: {info['cpu_percent']}%, RAM: {info['memory_percent']}%"
    
    def _update_stats(self, execution: AgentExecution):
        """Mettre √† jour les statistiques"""
        if execution.total_duration:
            # Moyenne mobile pour la dur√©e
            if self.stats["avg_duration"] == 0:
                self.stats["avg_duration"] = execution.total_duration
            else:
                self.stats["avg_duration"] = (self.stats["avg_duration"] + execution.total_duration) / 2
        
        # Moyenne mobile pour les √©tapes
        steps_count = len(execution.steps)
        if self.stats["avg_steps"] == 0:
            self.stats["avg_steps"] = steps_count
        else:
            self.stats["avg_steps"] = (self.stats["avg_steps"] + steps_count) / 2
    
    def get_stats(self) -> Dict:
        """Obtenir les statistiques de l'agent"""
        success_rate = (self.stats["successful_executions"] / self.stats["total_executions"]) * 100 if self.stats["total_executions"] > 0 else 0
        
        return {
            **self.stats,
            "success_rate": round(success_rate, 2),
            "is_active": self.is_active,
            "tools_count": len(self.tools)
        }
    
    async def _format_with_persona_async(self, content: str, context: Optional[Dict] = None) -> str:
        """
        Formater une r√©ponse avec la persona active
        
        Args:
            content: Contenu brut √† formater
            context: Contexte de la conversation
            
        Returns:
            Contenu format√© selon la persona active
        """
        if not self.persona_manager:
            return content
        
        try:
            # Enrichir le contexte avec des infos de l'agent
            enriched_context = context.copy() if context else {}
            enriched_context.update({
                "agent_type": "react_agent",
                "execution_context": True,
                "task_completion": True
            })
            
            # Sugg√©rer un changement de persona si appropri√©
            suggested_persona = await self.persona_manager.suggest_persona_switch(enriched_context)
            if suggested_persona:
                current_persona = self.persona_manager.get_current_persona()
                if current_persona and suggested_persona != current_persona.name:
                    logger.info(f"üí° Suggestion changement persona: {current_persona.name} ‚Üí {suggested_persona}")
                    # Note: Le changement automatique est d√©sactiv√© pour laisser le contr√¥le √† l'utilisateur
            
            # Formater avec la persona actuelle
            formatted = self.persona_manager.format_response(content, enriched_context)
            return formatted
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur formatage persona: {e}")
            return content
    
    async def get_persona_greeting(self) -> Optional[str]:
        """Obtenir un message de salutation de la persona active"""
        if not self.persona_manager:
            return None
        
        try:
            current_persona = self.persona_manager.get_current_persona()
            if current_persona:
                return current_persona.get_random_phrase("greetings")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur r√©cup√©ration salutation: {e}")
        
        return None
    
    async def get_persona_status_report(self) -> Optional[str]:
        """Obtenir un rapport de statut de la persona active"""
        if not self.persona_manager:
            return None
        
        try:
            current_persona = self.persona_manager.get_current_persona()
            if current_persona and hasattr(current_persona, 'get_status_report'):
                return current_persona.get_status_report()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur rapport persona: {e}")
        
        return None
    
    def _log_final_stats(self):
        """Logger les statistiques finales"""
        stats = self.get_stats()
        logger.info(f"üìä Agent Stats - Succ√®s: {stats['success_rate']}%, "
                   f"Dur√©e moy: {stats['avg_duration']:.2f}s, "
                   f"√âtapes moy: {stats['avg_steps']:.1f}")