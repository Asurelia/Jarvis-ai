# üèóÔ∏è JARVIS AI - Architecture Hybride Ollama: Guide Complet

## üìä Vue d'Ensemble de l'Architecture

### Architecture Actuelle vs. Architecture Cible

```mermaid
graph TB
    subgraph "AVANT - Architecture Docker Pure"
        A1[Frontend] --> B1[Brain API]
        B1 --> C1[Ollama Container]
        C1 --> D1[llama3.2:3b only]
        B1 --> E1[Memory DB]
        B1 --> F1[Redis]
    end
    
    subgraph "APR√àS - Architecture Hybride"
        A2[Frontend] --> B2[Brain API]
        B2 --> G2[LLM Gateway]
        G2 --> H2[Host Ollama]
        H2 --> I2[llama3.2:3b + gpt-oss-20b]
        G2 --> J2[Fallback Ollama Container]
        J2 --> K2[llama3.2:3b backup]
        L2[Network Monitor] --> G2
        B2 --> E2[Memory DB]
        B2 --> F2[Redis]
    end
    
    style H2 fill:#e8f5e8
    style I2 fill:#fff3cd
    style G2 fill:#e1f5fe
    style L2 fill:#f8d7da
```

## üéØ B√©n√©fices de l'Architecture Hybride

### Performance (+40% attendu)
- **Acc√®s direct GPU**: Ollama host utilise directement l'AMD RX 7800 XT
- **R√©duction latence r√©seau**: Pas de virtualisation Docker pour l'IA
- **Mod√®les multiples**: llama3.2:3b (rapide) + gpt-oss-20b (puissant)
- **Cache intelligent**: R√©ponses mises en cache selon la complexit√©

### R√©silience Enterprise
- **Failover automatique**: Host ‚Üí Container ‚Üí Cloud (futur)
- **Load balancing intelligent**: Routage selon la complexit√© des requ√™tes
- **Monitoring en temps r√©el**: D√©tection proactive des probl√®mes
- **Rollback garanti**: Retour √† l'architecture pr√©c√©dente en <5min

### √âvolutivit√© Cloud-Ready
- **Architecture microservices**: Services d√©coupl√©s et scalables
- **Multi-tenant ready**: Pr√©paration pour plusieurs clients
- **API Gateway pattern**: Point d'entr√©e centralis√© s√©curis√©
- **Observabilit√© compl√®te**: M√©triques, logs, traces distribu√©es

## üîß Composants de l'Architecture

### 1. LLM Gateway Service (Nouveau)
**Port**: 5010
**Responsabilit√©**: Orchestration intelligente des mod√®les LLM

```python
# Fonctionnalit√©s cl√©s
- Routage bas√© sur la complexit√© des requ√™tes
- Circuit breaker pour la r√©silience
- Cache intelligent des r√©ponses
- M√©triques de performance en temps r√©el
- Failover automatique Host ‚Üí Container
```

**Configuration**:
```yaml
llm-gateway:
  endpoints:
    primary: "http://host.docker.internal:11434"    # Host Ollama
    fallback: "http://ollama-fallback:11434"        # Container Ollama
    cloud: "${OPENROUTER_URL}"                      # Future cloud
  
  routing_strategy: "complexity_based"
  complexity_threshold: 0.7  # >0.7 = Large Model
  cache_ttl: 300  # 5 minutes
```

### 2. Network Monitor Service (Nouveau)
**Port**: 5011
**Responsabilit√©**: Monitoring connectivit√© Docker-Host

```python
# Surveillance en temps r√©el
- Connectivit√© host.docker.internal
- Sant√© des endpoints Ollama
- M√©triques de latence et disponibilit√©
- Alertes proactives sur les probl√®mes
- Historique des performances
```

### 3. Brain API (Modifi√©)
**Changements majeurs**:
- Support mode hybride via `OLLAMA_MODE=hybrid`
- Int√©gration LLM Gateway pour routage intelligent
- Configuration multi-mod√®les (llama3.2:3b + gpt-oss-20b)
- Timeouts adaptatifs pour Host vs Container

```python
# Nouvelle configuration
OLLAMA_MODE = "hybrid"  # single, hybrid, gateway_only
OLLAMA_PRIMARY_URL = "http://host.docker.internal:11434"
OLLAMA_FALLBACK_URL = "http://ollama-fallback:11434" 
LLM_GATEWAY_URL = "http://llm-gateway:5010"
```

## üåê Configuration R√©seau

### Docker Networks
```yaml
networks:
  jarvis_network:      # Services principaux
    subnet: 172.20.0.0/16
    
  jarvis_secure:       # Services internes (DB, Redis)
    internal: true
    subnet: 172.21.0.0/16
```

### Connectivity Matrix
| Service | Network | IP | Acc√®s Host |
|---------|---------|----|-----------| 
| Brain API | jarvis_network + jarvis_secure | 172.20.0.10 + 172.21.0.10 | ‚úÖ |
| LLM Gateway | jarvis_network + jarvis_secure | 172.20.0.140 + 172.21.0.140 | ‚úÖ |
| Network Monitor | jarvis_secure | 172.21.0.150 | ‚úÖ |
| Ollama Fallback | jarvis_network | 172.20.0.40 | ‚ùå |
| Memory DB | jarvis_secure | 172.21.0.60 | ‚ùå |
| Redis | jarvis_secure | 172.21.0.50 | ‚ùå |

## üìã Plan de D√©ploiement

### Phase 1: Pr√©paration (1-2h)
```bash
# 1. Validation de l'environnement
python scripts/migration-plan.py --status

# 2. Installation GPT-OSS 20B sur host
ollama pull gpt-oss-20b  # Si disponible

# 3. Backup de s√©curit√©
python scripts/migration-plan.py --backup

# 4. Build des nouveaux services
docker build -t jarvis-llm-gateway ./services/llm-gateway
docker build -t jarvis-network-monitor ./services/network-monitor
```

### Phase 2: D√©ploiement (30min)
```bash
# 1. Arr√™t services actuels
docker-compose down

# 2. D√©ploiement hybride
docker-compose -f docker-compose.hybrid-ollama.yml up -d

# 3. V√©rification sant√©
curl http://localhost:8080/health
curl http://localhost:5010/health  
curl http://localhost:5011/health
```

### Phase 3: Validation (30min)
```bash
# 1. Tests de connectivit√©
python scripts/test-hybrid-connectivity.py

# 2. Tests de performance
python scripts/performance-benchmark.py

# 3. Tests de failover
python scripts/test-failover-mechanism.py
```

### Phase 4: Monitoring (Continu)
```bash
# Dashboard de monitoring
open http://localhost:5011/status  # Network Monitor
open http://localhost:5010/stats   # LLM Gateway Stats
open http://localhost:8080/metrics # Brain API Metrics
```

## üîÑ Strat√©gies de Routage LLM

### 1. Complexity-Based Routing (Recommand√©)
```python
def select_model(prompt: str, complexity_score: float):
    """
    S√©lection automatique du mod√®le selon la complexit√©
    """
    if complexity_score >= 0.7:
        return "gpt-oss-20b"    # Mod√®le large pour t√¢ches complexes
    else:
        return "llama3.2:3b"    # Mod√®le rapide pour t√¢ches simples
        
# Indicateurs de complexit√©
complexity_indicators = [
    "analyze", "complex", "detailed", "comprehensive", 
    "code", "algorithm", "reasoning", "multi-step"
]
```

### 2. Performance-Based Routing
```python
def select_endpoint(endpoints: List[Endpoint]):
    """
    S√©lection bas√©e sur les performances actuelles
    """
    healthy_endpoints = [ep for ep in endpoints if ep.health_status]
    return min(healthy_endpoints, key=lambda x: x.avg_response_time)
```

### 3. Round-Robin avec Priorit√©
```python
def round_robin_with_priority(endpoints: List[Endpoint]):
    """
    Round-robin respectant les priorit√©s (Host > Container > Cloud)
    """
    sorted_endpoints = sorted(endpoints, key=lambda x: x.priority)
    return sorted_endpoints[request_count % len(sorted_endpoints)]
```

## üîê S√©curit√© et Isolation

### Niveaux de S√©curit√©
```yaml
# Niveau 1: Network Isolation
- R√©seau Docker internal pour services sensibles
- Firewall Windows pour limiter acc√®s Ollama host
- Communication chiffr√©e entre services

# Niveau 2: Application Security  
- JWT authentification sur tous les endpoints
- Rate limiting par utilisateur et IP
- Validation et sanitisation des inputs

# Niveau 3: System Hardening
- Containers non-root (user: "1000:1000")
- Pas de nouvelles privil√®ges (no-new-privileges:true)
- Read-only filesystems o√π possible
- Minimal attack surface (alpine images)
```

### Proc√©dure d'Incident
```bash
#!/bin/bash
# Emergency isolation script
echo "üö® Emergency: Isolating compromised services"

# Isoler du r√©seau host
docker network disconnect jarvis_network brain-api
docker network disconnect jarvis_network llm-gateway

# Basculer vers mode container uniquement
export OLLAMA_MODE=fallback_only
docker-compose restart brain-api

# Alerter l'√©quipe
curl -X POST $WEBHOOK_URL -d "ALERT: Host isolation activated"
```

## üìä M√©triques et KPIs

### Performance KPIs
- **Latence moyenne**: <500ms (vs 800ms avant)
- **Throughput**: >50 req/s (vs 35 req/s avant)  
- **Disponibilit√©**: >99.9% (failover automatique)
- **Temps de basculement**: <5s (Host ‚Üí Container)

### Business KPIs
- **Co√ªt GPU Cloud**: -60% (utilisation host)
- **Temps de r√©ponse utilisateur**: -40% 
- **Satisfaction utilisateur**: +25% (r√©ponses plus rapides)
- **Capacit√© de traitement**: +70% (mod√®les multiples)

## üõ†Ô∏è Maintenance et Troubleshooting

### Commandes de Diagnostic
```bash
# V√©rifier l'√©tat des services
docker-compose -f docker-compose.hybrid-ollama.yml ps

# Logs en temps r√©el
docker-compose -f docker-compose.hybrid-ollama.yml logs -f brain-api llm-gateway

# Test de connectivit√© host
docker run --rm --network jarvis_network curlimages/curl \
  curl -f http://host.docker.internal:11434/api/tags

# M√©triques de performance
curl -s http://localhost:5010/stats | jq '.endpoints'
```

### Probl√®mes Courants et Solutions
```yaml
Probl√®me: "Connection refused to host.docker.internal"
Solution: 
  - V√©rifier Docker Desktop "host.docker.internal" enabled
  - V√©rifier firewall Windows port 11434
  - Restart Docker Desktop service

Probl√®me: "High latency on host Ollama"  
Solution:
  - V√©rifier charge GPU avec rocm-smi
  - Ajuster OLLAMA_MAX_LOADED_MODELS
  - Activer le fallback container temporairement

Probl√®me: "Models not loading"
Solution:
  - V√©rifier espace disque disponible
  - Nettoyer anciens mod√®les: ollama rm old_model
  - Red√©marrer service Ollama host
```

## üöÄ Evolution Future

### Roadmap Q1 2025
- **Int√©gration OpenRouter**: Fallback cloud automatique
- **Auto-scaling**: D√©tection charge et scaling automatique  
- **Multi-GPU**: Support plusieurs GPUs en parall√®le
- **Edge deployment**: D√©ploiement sur edge devices

### Roadmap Q2 2025
- **Kubernetes migration**: Migration vers K8s pour production
- **Service mesh**: Istio pour communication s√©curis√©e
- **AI Model marketplace**: Hub de mod√®les internes
- **Multi-tenant**: Support clients multiples

Cette architecture hybride positionne JARVIS AI comme une solution enterprise moderne, performante et √©volutive, tout en conservant la simplicit√© d'utilisation et la fiabilit√© requises.