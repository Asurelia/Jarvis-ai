# ğŸ§ª Tests AutomatisÃ©s JARVIS

Ce rÃ©pertoire contient tous les scripts de test automatisÃ©s pour le systÃ¨me JARVIS AI, incluant les tests unitaires, d'intÃ©gration, de performance et end-to-end.

## ğŸ“‹ Structure des Tests

```
tests/
â”œâ”€â”€ test_new_components.py      # Tests Python des nouveaux modules
â”œâ”€â”€ test_ui_integration.js      # Tests d'intÃ©gration UI React
â”œâ”€â”€ requirements-test.txt       # DÃ©pendances Python pour les tests
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run-tests.sh           # Script principal de lancement
â”‚   â”œâ”€â”€ init-tests.sh          # Initialisation de l'environnement
â”‚   â””â”€â”€ generate_report.py     # GÃ©nÃ©rateur de rapports
â”œâ”€â”€ e2e/
â”‚   â”œâ”€â”€ specs/                 # Tests end-to-end Playwright
â”‚   â”œâ”€â”€ playwright.config.js   # Configuration Playwright
â”‚   â””â”€â”€ Dockerfile            # Image Docker pour E2E
â”œâ”€â”€ performance/
â”‚   â””â”€â”€ load_test.js          # Tests de charge K6
â””â”€â”€ results/                   # RÃ©sultats et rapports
```

## ğŸš€ DÃ©marrage Rapide

### 1. Tests Unitaires Simple

```bash
# Depuis la racine du projet JARVIS
cd tests
./scripts/run-tests.sh unit --verbose
```

### 2. Tests Complets avec Docker

```bash
# Tous les types de tests
./scripts/run-tests.sh all --build --clean --coverage --report

# Tests spÃ©cifiques
./scripts/run-tests.sh integration --verbose
./scripts/run-tests.sh performance --parallel
./scripts/run-tests.sh e2e --coverage
```

### 3. Tests Sans Docker (Local)

```bash
# Installation des dÃ©pendances
pip install -r requirements-test.txt

# Tests Python
python -m pytest test_new_components.py -v

# Tests JavaScript (depuis ui/)
cd ../ui && npm test -- test_ui_integration.js
```

## ğŸ“Š Types de Tests

### ğŸ§ª Tests Unitaires (`test_new_components.py`)

Teste l'importation et le fonctionnement des nouveaux modules JARVIS :

- **Modules Core** : Agent, AI, Vision, Voice, Control
- **WebWorkers** : Simulation du traitement parallÃ¨le
- **WASM** : Tests des modules WebAssembly simulÃ©s
- **Communication** : API, WebSocket, Redis, Ollama
- **Performance** : MÃ©triques de temps de rÃ©ponse

```python
# Exemple d'utilisation
from tests.test_new_components import TestModuleImports
test = TestModuleImports()
test.test_core_imports()
```

### ğŸ¨ Tests UI (`test_ui_integration.js`)

Teste les composants React et les interactions utilisateur :

- **Composants** : CognitiveIntelligenceModule, Sphere3D, ChatWindow
- **Interactions** : Clavier, souris, navigation
- **Performance** : Temps de rendu, utilisation mÃ©moire
- **CompatibilitÃ©** : Responsive, navigateurs

```javascript
// Exemple de test
import { render, screen } from '@testing-library/react';
import CognitiveIntelligenceModule from '../ui/src/components/CognitiveIntelligenceModule';

test('doit afficher les Ã©tats cognitifs', () => {
  render(<CognitiveIntelligenceModule />);
  expect(screen.getByText(/RÃ©flexion|Analyse/)).toBeInTheDocument();
});
```

### ğŸ¯ Tests E2E (`e2e/specs/`)

Tests end-to-end complets avec Playwright :

- **Interface** : Navigation, responsivitÃ©, accessibilitÃ©
- **FonctionnalitÃ©s** : Chat, voix, vision, modules avancÃ©s
- **IntÃ©gration** : WebSocket, API, services
- **Multi-plateforme** : Chrome, Firefox, Safari, Mobile

```javascript
// Exemple de test E2E
test('doit permettre une conversation complÃ¨te', async ({ page }) => {
  await page.goto('/');
  await page.click('[data-testid="chat-button"]');
  await page.fill('input', 'Bonjour JARVIS');
  await page.press('input', 'Enter');
  await expect(page.getByText(/rÃ©ponse/i)).toBeVisible();
});
```

### âš¡ Tests de Performance (`performance/`)

Tests de charge et performance avec K6 :

- **API** : Temps de rÃ©ponse, dÃ©bit, gestion des erreurs
- **WebSocket** : Connexions concurrentes, latence
- **UI** : Temps de chargement, animations
- **Services** : STT, TTS, contrÃ´le systÃ¨me

```javascript
// Exemple de test de charge
export default function() {
  const response = http.get('http://api:8080/health');
  check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 200ms': (r) => r.timings.duration < 200,
  });
}
```

## ğŸ³ Configuration Docker

### Docker Compose Test

Le fichier `docker-compose.test.yml` configure un environnement de test complet :

```yaml
services:
  # Services de test isolÃ©s
  brain-api-test:     # API principale (port 8081)
  test-memory-db:     # PostgreSQL test (port 5433)
  test-redis:         # Redis test (port 6380)
  test-ollama:        # Ollama lÃ©ger (port 11435)
  
  # Tests runners
  test-runner:        # Tests Python
  e2e-test:          # Tests Playwright
  performance-test:   # Tests K6
  security-test:     # Tests OWASP ZAP
```

### Profils Docker Compose

```bash
# Tests unitaires uniquement
docker-compose -f docker-compose.test.yml --profile unit-tests up

# Tests d'intÃ©gration
docker-compose -f docker-compose.test.yml --profile integration-tests up

# Tests complets E2E
docker-compose -f docker-compose.test.yml --profile full-tests up

# Tests de performance
docker-compose -f docker-compose.test.yml --profile perf-tests up

# Tests de sÃ©curitÃ©
docker-compose -f docker-compose.test.yml --profile security-tests up
```

## ğŸ›ï¸ Options du Script de Test

### `run-tests.sh [TYPE] [OPTIONS]`

**Types de Tests :**
- `unit` - Tests unitaires Python uniquement
- `integration` - Tests d'intÃ©gration des services
- `ui` - Tests de l'interface utilisateur React
- `e2e` - Tests end-to-end complets avec Playwright
- `performance` - Tests de performance et charge avec K6
- `security` - Tests de sÃ©curitÃ© avec OWASP ZAP
- `all` - Tous les tests (dÃ©faut)

**Options :**
- `--build` - Reconstruire les images Docker
- `--clean` - Nettoyer les volumes avant les tests
- `--verbose` - Mode verbose pour les logs dÃ©taillÃ©s
- `--parallel` - Lancer les tests en parallÃ¨le
- `--coverage` - GÃ©nÃ©rer un rapport de couverture de code
- `--report` - GÃ©nÃ©rer un rapport HTML consolidÃ©
- `--no-cache` - Ne pas utiliser le cache Docker

### Exemples d'Utilisation

```bash
# Tests de dÃ©veloppement rapides
./scripts/run-tests.sh unit --verbose

# Tests d'intÃ©gration avec reconstruction
./scripts/run-tests.sh integration --build --clean

# Tests E2E complets avec rapports
./scripts/run-tests.sh e2e --coverage --report

# Tests de performance en parallÃ¨le
./scripts/run-tests.sh performance --parallel

# Suite complÃ¨te pour CI/CD
./scripts/run-tests.sh all --build --clean --verbose --coverage --report
```

## ğŸ“Š Rapports et MÃ©triques

### GÃ©nÃ©ration de Rapports

```bash
# Rapport consolidÃ© automatique
./scripts/run-tests.sh all --report

# GÃ©nÃ©ration manuelle
python scripts/generate_report.py \
  --results ./results \
  --coverage ./coverage \
  --output ./results/consolidated-report.html
```

### Types de Rapports

1. **HTML ConsolidÃ©** : Vue d'ensemble avec mÃ©triques visuelles
2. **Coverage HTML** : Couverture de code dÃ©taillÃ©e par fichier
3. **Playwright HTML** : Rapports E2E interactifs avec traces
4. **K6 JSON** : MÃ©triques de performance dÃ©taillÃ©es
5. **JUnit XML** : Pour intÃ©gration CI/CD

### MÃ©triques SurveillÃ©es

- **Taux de RÃ©ussite** : Pourcentage de tests passÃ©s
- **Couverture de Code** : Lignes et branches couvertes
- **Performance** : Temps de rÃ©ponse, dÃ©bit, latence
- **AccessibilitÃ©** : Contraste, navigation clavier
- **SÃ©curitÃ©** : VulnÃ©rabilitÃ©s dÃ©tectÃ©es

## ğŸ”§ Configuration AvancÃ©e

### Variables d'Environnement

```bash
# URLs des services
export TEST_API_URL="http://localhost:8081"
export TEST_UI_URL="http://localhost:3001"
export TEST_DATABASE_URL="postgresql://postgres:test123@localhost:5433/jarvis_test"
export TEST_REDIS_URL="redis://localhost:6380"

# Configuration des tests
export PYTEST_ARGS="--verbose --tb=short --color=yes"
export PLAYWRIGHT_BROWSER="chromium"
export K6_VUS=10  # Utilisateurs virtuels
```

### Personnalisation des Tests

```python
# test_custom.py - Tests personnalisÃ©s
import unittest
from tests.test_new_components import TestModuleImports

class CustomJarvisTests(TestModuleImports):
    def test_custom_feature(self):
        # Votre test personnalisÃ©
        self.assertTrue(True)

if __name__ == '__main__':
    unittest.main()
```

## ğŸš¨ DÃ©pannage

### ProblÃ¨mes Courants

1. **Services non disponibles**
```bash
# VÃ©rifier les services
docker-compose -f docker-compose.test.yml ps
docker-compose -f docker-compose.test.yml logs brain-api-test
```

2. **Tests timeout**
```bash
# Augmenter les timeouts
export PYTEST_TIMEOUT=300
export PLAYWRIGHT_TIMEOUT=60000
```

3. **ProblÃ¨mes de permissions**
```bash
# Fixer les permissions
chmod +x scripts/*.sh
sudo chown -R $USER:$USER ./tests/results
```

4. **MÃ©moire insuffisante**
```bash
# Limiter les workers
export PYTEST_WORKERS=2
export PLAYWRIGHT_WORKERS=1
```

### Logs et Debug

```bash
# Logs dÃ©taillÃ©s
./scripts/run-tests.sh unit --verbose

# Debug Playwright
npx playwright test --debug

# Logs Docker
docker-compose -f docker-compose.test.yml logs --follow
```

## ğŸ“ˆ IntÃ©gration CI/CD

### GitHub Actions

```yaml
# .github/workflows/tests.yml
name: JARVIS Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Tests
        run: ./tests/scripts/run-tests.sh all --build --coverage --report
      - name: Upload Results
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: tests/results/
```

### Jenkins Pipeline

```groovy
pipeline {
    agent any
    stages {
        stage('Test') {
            steps {
                sh './tests/scripts/run-tests.sh all --build --coverage'
            }
            post {
                always {
                    publishHTML([
                        allowMissing: false,
                        alwaysLinkToLastBuild: true,
                        keepAll: true,
                        reportDir: 'tests/results',
                        reportFiles: 'consolidated-report.html',
                        reportName: 'JARVIS Test Report'
                    ])
                }
            }
        }
    }
}
```

## ğŸ—ï¸ DÃ©veloppement des Tests

### Ajouter de Nouveaux Tests

1. **Test Python**
```python
# Dans test_new_components.py
def test_my_new_feature(self):
    from core.my_module import MyClass
    instance = MyClass()
    self.assertEqual(instance.method(), "expected_value")
```

2. **Test React**
```javascript
// Dans test_ui_integration.js
test('mon nouveau composant', () => {
  render(<MonComposant />);
  expect(screen.getByText('Mon Texte')).toBeInTheDocument();
});
```

3. **Test E2E**
```javascript
// Dans e2e/specs/my-feature.spec.js
test('ma nouvelle fonctionnalitÃ©', async ({ page }) => {
  await page.goto('/ma-page');
  await expect(page.getByText('Ma FonctionnalitÃ©')).toBeVisible();
});
```

### Best Practices

- **Nommage** : Tests descriptifs en franÃ§ais
- **Isolation** : Chaque test doit Ãªtre indÃ©pendant
- **Cleanup** : Nettoyer les ressources aprÃ¨s les tests
- **Timeouts** : Utiliser des timeouts appropriÃ©s
- **Assertions** : VÃ©rifications spÃ©cifiques et claires

## ğŸ¤ Contribution

Pour contribuer aux tests :

1. CrÃ©er une branche feature : `git checkout -b feature/new-tests`
2. Ajouter vos tests avec documentation
3. Tester localement : `./scripts/run-tests.sh all --verbose`
4. CrÃ©er une PR avec les rÃ©sultats de tests

## ğŸ“ Support

Pour toute question ou problÃ¨me :

- ğŸ“§ Email : support@jarvis-ai.local
- ğŸ“– Documentation : `/docs`
- ğŸ› Issues : GitHub Issues
- ğŸ’¬ Chat : Canal #tests sur Discord

---

ğŸ¤– **Tests automatisÃ©s JARVIS** - Garantir la qualitÃ© et la fiabilitÃ© de l'IA